{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d447d9-2ed0-40e9-a16c-0d0bced5e812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09808736-f794-4bf0-bde7-f8a452df9de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"crop price Prediction\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6118102-02b8-4426-9a9d-77fbedaef539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- event_id: string (nullable = true)\n |-- event_time: string (nullable = true)\n |-- city: string (nullable = true)\n |-- crop: string (nullable = true)\n |-- market: string (nullable = true)\n |-- soil_moisture: string (nullable = true)\n |-- soil_ph: string (nullable = true)\n |-- rain_mm: string (nullable = true)\n |-- temp_c: string (nullable = true)\n |-- yield_kg: string (nullable = true)\n |-- pest_flag: string (nullable = true)\n |-- key: string (nullable = true)\n\n+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\n|            event_id|          event_time|     city|     crop|      market|     soil_moisture|           soil_ph|           rain_mm|temp_c|yield_kg|pest_flag|                 key|\n+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\n|358902c2-9064-49d...|2025-01-25T04:12:...|Bengaluru|     Rice|Yeshwanthpur| 18.35185578515821|7.1423795106084516|18.206146588044817|    22|     833|    False|Bengaluru|Rice|Ye...|\n|f62f6c3a-f04e-4d5...|2025-01-26T13:11:...|Bengaluru|    Wheat|   APMC_Yard| 27.59256148616627| 6.506742918454209| 44.68756167043308|    25|    4960|    False|Bengaluru|Wheat|A...|\n|c11e0dfd-e555-44a...|2025-01-03T02:53:...|    Delhi|    Wheat|       Okhla|7.9316311249474625| 6.013307197973608| 1.845755525791223|    12|    7979|    False|   Delhi|Wheat|Okhla|\n|9377f9da-73ce-469...|2025-01-30T07:38:...|   Mumbai|     Rice|       Dadar| 5.406082610704779| 6.259031482685412| 32.07795372309581|    42|    3206|    False|   Mumbai|Rice|Dadar|\n|be92a6a0-bc67-421...|2025-01-01T18:04:...|   Mumbai|Sugarcane|       Dadar| 18.39322162093865| 5.481994585377222| 10.86139235717659|    33|    5210|    False|Mumbai|Sugarcane|...|\n+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Replace with your dataset path\n",
    "data_path = \"/Volumes/workspace/default/market_data/data.csv\"\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", False).csv(data_path)\n",
    "\n",
    "# Check schema\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd8af18-1255-49bb-9bc1-e2294d1fce59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numeric columns (features + label)\n",
    "numeric_cols = ['temp_c', 'soil_ph', 'rain_mm', 'yield_kg']\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = ['city', 'market', 'crop']  # update as per your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a149159-3c65-476c-bb47-1b7a8331dc6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\n|            event_id|          event_time|     city|     crop|      market|     soil_moisture|           soil_ph|           rain_mm|temp_c|yield_kg|pest_flag|                 key|\n+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\n|358902c2-9064-49d...|2025-01-25T04:12:...|Bengaluru|     Rice|Yeshwanthpur| 18.35185578515821|7.1423795106084516|18.206146588044817|  22.0|   833.0|    False|Bengaluru|Rice|Ye...|\n|f62f6c3a-f04e-4d5...|2025-01-26T13:11:...|Bengaluru|    Wheat|   APMC_Yard| 27.59256148616627| 6.506742918454209| 44.68756167043308|  25.0|  4960.0|    False|Bengaluru|Wheat|A...|\n|c11e0dfd-e555-44a...|2025-01-03T02:53:...|    Delhi|    Wheat|       Okhla|7.9316311249474625| 6.013307197973608| 1.845755525791223|  12.0|  7979.0|    False|   Delhi|Wheat|Okhla|\n|9377f9da-73ce-469...|2025-01-30T07:38:...|   Mumbai|     Rice|       Dadar| 5.406082610704779| 6.259031482685412| 32.07795372309581|  42.0|  3206.0|    False|   Mumbai|Rice|Dadar|\n|be92a6a0-bc67-421...|2025-01-01T18:04:...|   Mumbai|Sugarcane|       Dadar| 18.39322162093865| 5.481994585377222| 10.86139235717659|  33.0|  5210.0|    False|Mumbai|Sugarcane|...|\n+--------------------+--------------------+---------+---------+------------+------------------+------------------+------------------+------+--------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Only keep numeric values, drop malformed strings like 'False', 'Rice', etc.\n",
    "for c in numeric_cols:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        when(col(c).rlike(\"^[+-]?([0-9]*[.])?[0-9]+$\"), col(c).cast(\"double\")).otherwise(None)\n",
    "    )\n",
    "\n",
    "# Drop rows with NULLs in any numeric column\n",
    "df = df.dropna(subset=numeric_cols)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "891bd48f-5690-428d-8130-b8f3e775a4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n|            features|yield_kg|\n+--------------------+--------+\n|(24,[0,1,2,3,8,21...|   833.0|\n|(24,[0,1,2,3,7,22...|  4960.0|\n|(24,[0,1,2,6,13,2...|  7979.0|\n|(24,[0,1,2,4,12,2...|  3206.0|\n|(24,[0,1,2,4,12,2...|  5210.0|\n+--------------------+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\") for c in categorical_cols]\n",
    "\n",
    "# Assemble feature columns\n",
    "feature_cols = [c for c in numeric_cols if c != 'yield_kg'] + [c+\"_vec\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Fit and transform dataset\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df.select('features', 'yield_kg').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e388ca79-1f20-44a2-bddd-d014b81f80b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99240a85-51c7-4461-9db8-519955cd8776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='yield_kg')\n",
    "model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ddbd76-27ca-4e1d-9e74-f1281a26d91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n|yield_kg|        prediction|\n+--------+------------------+\n|  5420.0|3905.2220403208566|\n|  3336.0| 3914.865871230568|\n|  2112.0| 3913.567709652408|\n|   303.0|3928.1131542602056|\n|  3379.0|3902.6100208247644|\n+--------+------------------+\nonly showing top 5 rows\nRMSE: 2283.5124100625417\nR2: 3.512244040504431e-05\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"yield_kg\", \"prediction\").show(5)\n",
    "\n",
    "# Optional: Model metrics\n",
    "trainingSummary = model.summary\n",
    "print(\"RMSE:\", trainingSummary.rootMeanSquaredError)\n",
    "print(\"R2:\", trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded79e46-9fac-43b3-93d7-a47be51a6d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully\nAvailable columns: ['event_id', 'event_time', 'city', 'crop', 'market', 'soil_moisture', 'soil_ph', 'rain_mm', 'temp_c', 'yield_kg', 'pest_flag', 'key']\n✅ Cleaned data (numeric columns casted properly)\n+------------------+------+--------+\n|           rain_mm|temp_c|yield_kg|\n+------------------+------+--------+\n|18.206146588044817|  22.0|   833.0|\n| 44.68756167043308|  25.0|  4960.0|\n| 1.845755525791223|  12.0|  7979.0|\n| 32.07795372309581|  42.0|  3206.0|\n| 10.86139235717659|  33.0|  5210.0|\n+------------------+------+--------+\nonly showing top 5 rows\n✅ Model trained successfully\n\uD83D\uDCC8 R² Score: -0.0001\n\uD83D\uDCC9 RMSE: 2287.8152\n Model saved successfully at /Volumes/workspace/default/market_data/agri_yield_lr_model\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# \uD83C\uDF3E Agricultural Yield Prediction - Clean & Train Version\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, regexp_replace, when, isnan\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣  Create Spark session\n",
    "# ------------------------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"CropYieldPredictor\").getOrCreate()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣  Load data safely\n",
    "# ------------------------------------------------------------\n",
    "path = \"/Volumes/workspace/default/market_data/data.csv\"  # update if needed\n",
    "df = spark.read.option(\"header\", True).csv(path)\n",
    "print(\"✅ Data loaded successfully\")\n",
    "print(\"Available columns:\", df.columns)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣  Clean up numeric columns (remove invalid strings like 'False')\n",
    "# ------------------------------------------------------------\n",
    "numeric_cols = [\"soil_moisture\", \"soil_ph\", \"rain_mm\", \"temp_c\", \"yield_kg\"]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    # Replace non-numeric values (like 'False', 'NA', '') with NULL\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        when(\n",
    "            col(c).rlike(\"^[0-9.]+$\"),  # keep only valid numbers\n",
    "            col(c)\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    # Cast to double\n",
    "    df = df.withColumn(c, col(c).cast(DoubleType()))\n",
    "\n",
    "print(\"✅ Cleaned data (numeric columns casted properly)\")\n",
    "df.select(\"rain_mm\", \"temp_c\", \"yield_kg\").show(5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣  Drop rows with nulls in important numeric fields\n",
    "# ------------------------------------------------------------\n",
    "df = df.na.drop(subset=numeric_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣  Feature engineering\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = [\"soil_moisture\", \"soil_ph\", \"rain_mm\", \"temp_c\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣  Split train/test\n",
    "# ------------------------------------------------------------\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣  Train model\n",
    "# ------------------------------------------------------------\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"yield_kg\",\n",
    "    maxIter=10,\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "model = lr.fit(train_data)\n",
    "print(\"✅ Model trained successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣  Evaluate model\n",
    "# ------------------------------------------------------------\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"yield_kg\", predictionCol=\"prediction\")\n",
    "\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "print(f\"\uD83D\uDCC8 R² Score: {r2:.4f}\")\n",
    "print(f\"\uD83D\uDCC9 RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣  Save model (optional)\n",
    "# ------------------------------------------------------------\n",
    "model.save(\"/Volumes/workspace/default/market_data/agri_yield_lr_model\")\n",
    "print(\" Model saved successfully at /Volumes/workspace/default/market_data/agri_yield_lr_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6f16db-d1fa-4692-a267-8a64219e3efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ new_product.csv created successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"crop\", StringType(), True),\n",
    "    StructField(\"market\", StringType(), True),\n",
    "    StructField(\"soil_moisture\", StringType(), True),\n",
    "    StructField(\"soil_ph\", StringType(), True),\n",
    "    StructField(\"rain_mm\", StringType(), True),\n",
    "    StructField(\"temp_c\", StringType(), True),\n",
    "    StructField(\"yield_kg\", StringType(), True),\n",
    "    StructField(\"pest_flag\", StringType(), True),\n",
    "    StructField(\"key\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"E001\", \"2025-10-16 10:00\", \"CityA\", \"Wheat\", \"Market1\", \"0.3\", \"6.5\", \"25.0\", \"30.0\", \"5000\", \"0\", \"K001\"),\n",
    "    (\"E002\", \"2025-10-16 11:00\", \"CityB\", \"Rice\", \"Market2\", \"0.4\", \"6.8\", \"40.0\", \"32.0\", \"6000\", \"1\", \"K002\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Save to your volume\n",
    "df.write.csv(\"/Volumes/workspace/default/market_data/new_product.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ new_product.csv created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a055e04-5d0a-4334-8ed4-3ea00c214f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New data loaded successfully\nColumns: ['event_id', 'event_time', 'city', 'crop', 'market', 'soil_moisture', 'soil_ph', 'rain_mm', 'temp_c', 'yield_kg', 'pest_flag', 'key']\n+--------+-------------------+-----+-----+-------+-------------+-------+-------+------+--------+---------+----+\n|event_id|         event_time| city| crop| market|soil_moisture|soil_ph|rain_mm|temp_c|yield_kg|pest_flag| key|\n+--------+-------------------+-----+-----+-------+-------------+-------+-------+------+--------+---------+----+\n|    E001|2025-10-16 10:00:00|CityA|Wheat|Market1|          0.3|    6.5|   25.0|  30.0|    5000|        0|K001|\n|    E002|2025-10-16 11:00:00|CityB| Rice|Market2|          0.4|    6.8|   40.0|  32.0|    6000|        1|K002|\n+--------+-------------------+-----+-----+-------+-------------+-------+-------+------+--------+---------+----+\n\n+--------+-----+-------+------------------+\n|event_id| crop| market|        prediction|\n+--------+-----+-------+------------------+\n|    E001|Wheat|Market1| 3909.222742136389|\n|    E002| Rice|Market2|3912.3059472572713|\n+--------+-----+-------+------------------+\n\n✅ Predictions saved at /Volumes/workspace/default/market_data/predicted_trends.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Import required libraries\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Initialize Spark session (if not already)\n",
    "# ------------------------------------------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Load new data for prediction\n",
    "# ------------------------------------------------------------\n",
    "new_data_path = \"/Volumes/workspace/default/market_data/new_product.csv\"\n",
    "\n",
    "new_data = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(new_data_path)\n",
    "print(\"✅ New data loaded successfully\")\n",
    "print(\"Columns:\", new_data.columns)\n",
    "new_data.show(5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Convert columns to float/double if needed\n",
    "# ------------------------------------------------------------\n",
    "numeric_cols = [\"soil_moisture\", \"soil_ph\", \"rain_mm\", \"temp_c\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    new_data = new_data.withColumn(col, new_data[col].cast(\"double\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Assemble features\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = [\"soil_moisture\", \"soil_ph\", \"rain_mm\", \"temp_c\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "new_data_prepared = assembler.transform(new_data)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Load the trained Linear Regression model\n",
    "# ------------------------------------------------------------\n",
    "lr_model_path = \"/Volumes/workspace/default/market_data/agri_yield_lr_model\"\n",
    "lr_model = LinearRegressionModel.load(lr_model_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Make predictions\n",
    "# ------------------------------------------------------------\n",
    "predictions = lr_model.transform(new_data_prepared)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ Show predictions\n",
    "# ------------------------------------------------------------\n",
    "predictions.select(\"event_id\", \"crop\", \"market\", \"prediction\").show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣ Optionally save predictions\n",
    "# ------------------------------------------------------------\n",
    "predicted_output_path = \"/Volumes/workspace/default/market_data/predicted_trends.csv\"\n",
    "predictions.select(\"event_id\", \"crop\", \"market\", \"prediction\")\\\n",
    "    .write.mode(\"overwrite\").option(\"header\", True).csv(predicted_output_path)\n",
    "\n",
    "print(f\"✅ Predictions saved at {predicted_output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9ed2ce-0f7e-4d23-a55d-a30a2b5a4872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+------------------+\n|event_id| crop| market|        prediction|\n+--------+-----+-------+------------------+\n|    E001|Wheat|Market1| 3909.222742136389|\n|    E002| Rice|Market2|3912.3059472572713|\n+--------+-----+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the predictions CSV\n",
    "pred_df = spark.read.option(\"header\", True).csv(\"/Volumes/workspace/default/market_data/predicted_trends.csv\")\n",
    "\n",
    "# Show the top rows\n",
    "pred_df.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5853083104629839,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Big_data_new2025-10-16 00:39:24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}